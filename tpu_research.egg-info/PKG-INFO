Metadata-Version: 2.4
Name: tpu-research
Version: 0.1.0
Summary: Reference library for TPU hardware characteristics and JAX feature support
License: Apache-2.0
Keywords: TPU,JAX,machine learning,hardware
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Provides-Extra: jax-cpu
Requires-Dist: jax[cpu]>=0.4.0; extra == "jax-cpu"
Provides-Extra: jax-tpu
Requires-Dist: jax[tpu]>=0.4.0; extra == "jax-tpu"
Provides-Extra: test
Requires-Dist: pytest>=7.0; extra == "test"

# tpu-research

A research reference covering Google TPU hardware characteristics and JAX feature support.

## Table of Contents

- [TPU Hardware Overview](#tpu-hardware-overview)
- [JAX Feature Support on TPU](#jax-feature-support-on-tpu)
- [Repository Structure](#repository-structure)
- [Getting Started](#getting-started)
- [Examples](#examples)

---

## TPU Hardware Overview

### Generations

| Generation | Peak BF16 TFLOPS (per chip) | HBM Capacity | Interconnect |
|---|---|---|---|
| TPU v2 | 45 | 8 GiB (4 × 2 GiB HBM) | 2D torus |
| TPU v3 | 123 | 16 GiB (4 × 4 GiB HBM) | 2D torus |
| TPU v4 | 275 | 32 GiB (4 × 8 GiB HBM) | 3D torus (OCS) |
| TPU v5e | 197 | 16 GiB | 2D torus |
| TPU v5p | 459 | 95 GiB | 3D torus |

### Key Hardware Units

#### Matrix Multiply Unit (MXU)
- The core compute engine of every TPU chip.
- Performs 128 × 128 matrix multiplications in **BFloat16** → **Float32** accumulation each cycle.
- Utilisation is maximised when operand shapes are multiples of 128 (the *preferred padding size*).

#### Scalar Unit & Vector Unit
- The **scalar unit** handles control flow, loop counters and address arithmetic.
- The **vector unit** (VPU) executes element-wise operations (activation functions, normalisation, etc.) at 128-element granularity.

#### High Bandwidth Memory (HBM)
- Each TPU core has dedicated HBM sitting next to the die for low-latency, high-bandwidth access.
- Memory bandwidth is typically 10–30× higher than PCIe-attached DRAM on a CPU server.

#### On-Chip Memory (VMEM / SMEM)
- A small scratchpad SRAM (typically 16–32 MiB) directly accessible by the MXU and VPU.
- Moving data from HBM → VMEM before compute avoids repeated off-chip traffic and is key to kernel efficiency.

#### Inter-Chip Interconnect (ICI)
- TPU pods are connected by a dedicated, ultra-low-latency ICI fabric (2D or 3D torus).
- All-reduce collective operations across hundreds of chips execute without going through host CPUs or a NIC.

### Numerical Formats

| Format | Exponent bits | Mantissa bits | Notes |
|---|---|---|---|
| BFloat16 | 8 | 7 | Native TPU format; same dynamic range as FP32 |
| Float32 | 8 | 23 | Used for accumulation in MXU |
| Int8 | — | 8 | Supported on v4+ for inference |
| Float16 | 5 | 10 | Supported but BF16 preferred |

---

## JAX Feature Support on TPU

### Compilation

| JAX API | TPU behaviour |
|---|---|
| `jax.jit` | Traces Python → StableHLO → XLA HLO → TPU binary (via XLA compiler). Runs fully on-device. |
| `jax.lax.*` | All lax primitives lower to XLA HLO ops with first-class TPU support. |
| `jax.pure_callback` | Supported with caveats; prefer native JAX ops for performance. |

### Automatic Differentiation

| API | Notes |
|---|---|
| `jax.grad` / `jax.value_and_grad` | Full support; VJPs generated by XLA. |
| `jax.jacfwd` / `jax.jacrev` | Supported; `jacfwd` uses forward-mode AD via `jvp`. |
| `jax.hessian` | Supported (forward-over-reverse). |

### Vectorisation & Parallelism

| API | Description |
|---|---|
| `jax.vmap` | Batches over an axis; lowers to efficient XLA batch dimensions — no manual loop needed. |
| `jax.pmap` | Single-program multiple-data (SPMD) over TPU cores; uses ICI all-reduce for collective ops. |
| `jax.sharding` / `jax.experimental.mesh_utils` | Modern API for explicit device mesh + sharding annotations (successor to `pmap`). |
| `jax.lax.ppermute` / `jax.lax.psum` | Low-level collective communication primitives mapped directly to ICI operations. |

### Sharding & Distributed Training (GSPMD)

JAX uses the **GSPMD** (Globally Sharded Parallel Model Decomposition) paradigm:

1. Annotate arrays with `jax.sharding.PartitionSpec` or `jax.sharding.NamedSharding`.
2. `jit`-compile the function — XLA automatically inserts collective operations.
3. The compiler partitions computation across the device mesh without explicit host coordination.

```python
import jax
import jax.numpy as jnp
from jax.sharding import Mesh, PartitionSpec as P, NamedSharding
from jax.experimental import mesh_utils

# Create a 2-D mesh of all available devices
devices = mesh_utils.create_device_mesh((2, 4))  # 2 rows × 4 cols
mesh = Mesh(devices, axis_names=("batch", "model"))

# Shard a weight matrix along the model axis
sharding = NamedSharding(mesh, P(None, "model"))
W = jax.device_put(jnp.ones((128, 512)), sharding)
```

### TPU-Specific Best Practices in JAX

1. **Use BFloat16** — cast weights/activations to `jnp.bfloat16` before computation. The MXU is native BF16; FP32 throughput is ~4× lower.
2. **Pad to multiples of 128** — matrix dimensions that are multiples of 128 avoid MXU padding overhead.
3. **Minimise host ↔ device transfers** — every `jax.device_get` / `jnp.array(host_array)` incurs PCIe latency. Keep data on-device.
4. **Prefer `jnp` over Python loops** — Python-level loops produce large HLO graphs; vectorise with `vmap` or write `lax.scan`/`lax.while_loop`.
5. **Use `lax.scan` for sequential loops** — avoids unrolling; keeps compiled binary size small.
6. **Profile with `jax.profiler`** — attach a TensorBoard trace to find MXU utilisation, memory bottlenecks and communication overhead.

---

## Repository Structure

```
tpu-research/
├── README.md
├── tpu_features/
│   ├── __init__.py
│   ├── hardware.py          # TPU hardware characteristic constants & helpers
│   ├── compilation.py       # JAX JIT / AOT compilation utilities
│   ├── parallelism.py       # pmap / sharding / mesh helpers
│   └── optimizations.py     # BF16 casting, padding, memory-layout helpers
├── examples/
│   ├── matmul_bfloat16.py   # BF16 matrix multiply benchmark
│   ├── pmap_allreduce.py     # Multi-chip all-reduce with pmap
│   └── sharding_example.py  # Named sharding on a device mesh
└── tests/
    ├── test_hardware.py
    ├── test_compilation.py
    ├── test_parallelism.py
    └── test_optimizations.py
```

---

## Getting Started

```bash
pip install "jax[tpu]" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
git clone https://github.com/0xaskr/tpu-research.git
cd tpu-research
python -m pytest tests/
```

---

## Examples

```python
# Quick BF16 matmul — achieves near-peak MXU utilisation
import jax
import jax.numpy as jnp
from tpu_features.optimizations import pad_to_multiple, cast_bfloat16

A = jnp.ones((1024, 1024))
B = jnp.ones((1024, 1024))

A_bf16 = cast_bfloat16(pad_to_multiple(A, multiple=128))
B_bf16 = cast_bfloat16(pad_to_multiple(B, multiple=128))

result = jax.jit(jnp.matmul)(A_bf16, B_bf16)
print(result.shape)  # (1024, 1024)
```

See the `examples/` directory for more runnable notebooks and scripts.
